{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# December 16, 2018\n",
    "## Databases\n",
    "I've been investigating how to output the data into databases. I found that relational databases aren't really the best place to store these types of data. These data are most definitely time series data, and they should be treated as such. [Time Series Databases](https://en.wikipedia.org/wiki/Time_series_database) provide an excellent starting point to look at this. \n",
    "\n",
    "## Database Choice\n",
    "I chose to use [Timescale](https://www.timescale.com/) as our time series database. It's based on [PostgreSQL](https://www.postgresql.org/docs/10/index.html), which provides us with some advantages over other databases. It allows us to store JSON documents (e.g. header/firmware information). We're able to set the time column to be something other than a date (e.g. an integer). It might not be as redundant as something like MapR, but it's a good start. \n",
    "\n",
    "## Setup Timescale DB\n",
    "I followed the Ubuntu [install instructions](https://docs.timescale.com/v1.1/getting-started/installation/ubuntu/installation-apt-ubuntu). Logging into the cli with\n",
    "```bash\n",
    "sudo service postgresql restart\n",
    "psql -U postgres -W\n",
    "```\n",
    "Then creating the database\n",
    "```sql\n",
    "# create the database\n",
    "CREATE database bagel;\n",
    "\n",
    "# load the database\n",
    "\\c bagel\n",
    "\n",
    "# convert to a timescaledb\n",
    "create extension if not exists timescaledb cascade;\n",
    "\n",
    "# shutoff telemetry tracking\n",
    "ALTER [SYSTEM | DATABASE | USER] { *db_name* | *role_specification* } SET timescaledb.telemetry_level=off\n",
    "\n",
    "# Create the table\n",
    "CREATE TABLE data (\n",
    "  id        smallint          NOT NULL,\n",
    "  energy    DOUBLE PRECISION  NOT NULL,\n",
    "  time      bigint            NOT NULL,\n",
    "  cfd_time  DOUBLE PRECISION  NOT NULL\n",
    ");\n",
    "\n",
    "# create the hypertable for data table using the time column. Chunk time interval is set to 1 day for unix epoch time in ms. \n",
    "# Will need to think more about how this works. \n",
    "SELECT create_hypertable('data', 'time', chunk_time_interval => 86400000);\n",
    "\n",
    "# Inserting a data point\n",
    "INSERT INTO DATA VALUES(14,304.08124800000000398,19606962062518.000777,39213924125036.353274);\n",
    "```\n",
    "\n",
    "### Creating Users\n",
    "```sql\n",
    "create user vincent with password 'password';\n",
    "grant all privileges on database bagel to vincent;\n",
    "```\n",
    "\n",
    "## Uploading data from sqlite\n",
    "I uploaded the data from the sqlite by outputting as a CSV, truncating the decimal from the time column, and importing into the data table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# December 17, 2018\n",
    "## Discussion w/ George\n",
    "I had a discussion w/ George about the databases and how to setup the tables. I have a couple of options here. Having just a single big table might work for a while. His recommendation was that we have several tables. One table has {timestamps, crate, module, channel}. The other channel has all the details. That allows us to do quick lookups for what we need from the main table and then cross reference them to the details table. \n",
    "\n",
    "## Installing postgresql for windows\n",
    "I'm going to install the database on windows as well. That way I can connect to the linux box without issues. I'm thinking that we might be able to connect w/o the database installed by using python. Let's check. \n",
    "\n",
    "## Testing the database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14, 7576.572214, 19693238423165, 39386476846330.4), (14, 417.343303, 19693238484790, 39386476969580.0), (14, 305.573827, 19693238637156, 39386477274312.0), (14, 80.61771, 19693238698727, 39386477397454.5), (14, 342.357624, 19693238755370, 39386477510740.1), (14, 326.773797, 19693238906728, 39386477813456.2), (14, 1738.296982, 19693238914296, 39386477828592.8), (14, 232.901547, 19693238998831, 39386477997662.1), (14, 122.707598, 19693239018862, 39386478037724.8), (14, 135.301811, 19693239025359, 39386478050718.4), (14, 229.683297, 19693239082540, 39386478165080.2), (14, 561.489045, 19693239112649, 39386478225298.6), (14, 584.930979, 19693239202662, 39386478405324.2), (14, 1840.136679, 19693239228775, 39386478457550.2), (14, 6171.735934, 19693239298645, 39386478597290.2), (14, 160.691131, 19693239301822, 39386478603644.3), (14, 1532.055122, 19693239349315, 39386478698630.7), (14, 218.15213, 19693239404802, 39386478809604.4), (14, 869.480467, 19693239502743, 39386479005486.1), (14, 95.535141, 19693239503647, 19693239503647.0)]\n"
     ]
    }
   ],
   "source": [
    "import keyring\n",
    "import psycopg2\n",
    "import yaml\n",
    "\n",
    "with open('../cfg.yaml') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "    \n",
    "conn = psycopg2.connect(user=cfg['username'], password=keyring.get_password(cfg['host'], cfg['username']),\n",
    "                                      host=cfg['host'], port=cfg['port'], database=cfg['db'])\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('select * from data where id=14 limit 20')\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HA!! It works!** Now we just need to figure out how to make this all more efficient. Right now it takes a long time to get the data out for a single clover channel. I'm going to try copying them out to another table and executing the same query.\n",
    "\n",
    "## Copy values from one table to another\n",
    "```sql\n",
    "CREATE TABLE id04(id smallint NOT NULL, energy DOUBLE PRECISION NOT NULL, time bigint, cfd_time DOUBLE PRECISION NOT NULL);\n",
    "INSERT INTO Table2 SELECT * FROM Table1 WHERE [Conditions]\n",
    "```\n",
    "Whelp, that didn't fare much better than the previous attempt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# December 29, 2018\n",
    "## Test tables\n",
    "Our test trigger time will be 19623818309088. We can find the gated gammas with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 3401.897553, 19623818308976, 39247636617952.7), (9, 1284.855562, 19623818308966, 39247636617932.0)]\n"
     ]
    }
   ],
   "source": [
    "cursor = connection.cursor()\n",
    "cursor.execute('select * from data where time between 19623818309088-126 and 19623818309088-63;')\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a table to hold results:\n",
    "```sql\n",
    "create table gated (id int, ge_energy double precision, ge_time bigint);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# December 30, 2018\n",
    "## Slow as dirt\n",
    "Things still are not too performant. Iâ€™m looking at 1000 trigger times and it takes about 5 min to generate our list. According to [this](https://stackoverflow.com/questions/8134602/psycopg2-insert-multiple-rows-with-one-query) forum we could see an improvement by generating the query ourself and executing it.\n",
    "## Generating our own query\n",
    "That does seem to have speeded things up a little bit. Let's try giving it the whole dataset. Generating the full statement list for 10 threads takes about 20 seconds each. Executing the queries ended in failure. I will need to determine a different way to execute this query. Error message says: \n",
    "> psycopg2.OperationalError: no connection to the server\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# December 31, 2018\n",
    "## Simple Kafka Cosumer\n",
    "We're going to start working on our simple Kafka Consumer/Producer while it's fresh on my mind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# January 1, 2019\n",
    "## Setting up Kafka on Windows\n",
    "Following these [instructions](https://kafka.apache.org/quickstart). Using [Java SE Runtime 1.8.192](https://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html)\n",
    "\n",
    "```\n",
    "# Starts the Zookeeper server \n",
    "D:\\Programs\\kafka_2.11-2.1.0\\bin\\windows\\zookeeper-server-start.bat D:\\Programs\\kafka_2.11-2.1.0\\config\\zookeeper.properties\n",
    "\n",
    "# Starts the actual Kafka server with the provided properties file.\n",
    "D:\\Programs\\kafka_2.11-2.1.0\\bin\\windows\\kafka-server-start.bat D:\\Programs\\kafka_2.11-2.1.0\\config\\server.properties\n",
    "\n",
    "# Created a topic named test with replication 1, and a single partition. \n",
    "D:\\Programs\\kafka_2.11-2.1.0\\bin\\windows\\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test\n",
    "\n",
    "# Starts the console based producer\n",
    "D:\\Programs\\kafka_2.11-2.1.0\\bin\\windows\\kafka-console-producer.bat --broker-list localhost:9092 --topic test\n",
    "\n",
    "# Starts the console based consumer\n",
    "D:\\Programs\\kafka_2.11-2.1.0\\bin\\windows\\kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic test --from-beginning\n",
    "```\n",
    "**NOTE: We need to have more kafka brokers (e.g. running instances of kafka) than our desired replication factors.**\n",
    "\n",
    "I created a topic called daq to start testing communication between the producer and consumer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# January 2, 2019\n",
    "## SQL efficiency\n",
    "We could boost our efficiency finding overlapping events by doing a join on the main data table. The image below describes this process at a high level. \n",
    "![ansi join](ansi-join.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# January 5, 2019\n",
    "## Consumer Working\n",
    "The consumer is now consuming. We needed to set the bootstrap server to `localhost` so that it knew where the topics were located. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# January 7, 2019\n",
    "## Working on a c++ producer\n",
    "Got the simple producer example working and able to read from the topic. Expanding it out now to use PAASS-LC Skeleton. Running into some linking issues. \n",
    "```\n",
    "CMakeFiles/skeleton.dir/SkeletonUnpacker.cpp.o: In function `SkeletonUnpacker::ProcessRawEvent()':\n",
    "SkeletonUnpacker.cpp:(.text+0x2bf): undefined reference to `cppkafka::Buffer::Buffer()'\n",
    "SkeletonUnpacker.cpp:(.text+0x2c8): undefined reference to `cppkafka::Buffer::Buffer()'\n",
    "collect2: error: ld returned 1 exit status\n",
    "Analysis/Utilities/Skeleton/source/CMakeFiles/skeleton.dir/build.make:120: recipe for target 'Analysis/Utilities/Skeleton/source/skeleton' failed\n",
    "make[3]: *** [Analysis/Utilities/Skeleton/source/skeleton] Error 1\n",
    "CMakeFiles/Makefile2:829: recipe for target 'Analysis/Utilities/Skeleton/source/CMakeFiles/skeleton.dir/all' failed\n",
    "make[2]: *** [Analysis/Utilities/Skeleton/source/CMakeFiles/skeleton.dir/all] Error 2\n",
    "CMakeFiles/Makefile2:841: recipe for target 'Analysis/Utilities/Skeleton/source/CMakeFiles/skeleton.dir/rule' failed\n",
    "make[1]: *** [Analysis/Utilities/Skeleton/source/CMakeFiles/skeleton.dir/rule] Error 2\n",
    "Makefile:320: recipe for target 'skeleton' failed\n",
    "make: *** [skeleton] Error 2\n",
    "```\n",
    "DUH, it's b/c I'm not linking to `libcppkafka`. Now we're failing with exit code 127. Good old exit code 127. Also, screw you clion for not showing my real errors. The SO isn't found during runtime:\n",
    ">/home/vincent/projects/paass-lc/repo/cmake-build-release/Analysis/Utilities/Skeleton/source/skeleton: error while loading shared libraries: libcppkafka.so.0.2: cannot open shared object file: No such file or directory\n",
    "\n",
    "Will need to get that listed in the `LD_LIBRARY_PATH`. That's all working and we're seeing information coming through in Kafka now!\n",
    "```json\n",
    "{\n",
    "  'time': 2019-01-07 17:29:58,177,\n",
    "  'module_name': ConsumerWorker,\n",
    "  'level': INFO,\n",
    "  'pid': 5604,\n",
    "  'thread': Thread-1,\n",
    "  'loggerName': root,\n",
    "  'message': Writing from PAASS-LC Skeleton - Processing buffer from module 1 of length 5348\n",
    "}\n",
    "```\n",
    "We're going to need to figure out how to pass the whole data block (sans first two words) to Kafka. that's going to be the closest emulation of what we'll get from the module. If we figure out how to handle these in our consumer, then we should be in good shape. This will also give us an idea of how to handle it in the producer as well. Figured out how to send the binary data using a string, but having trouble getting it through intact on the other side. Will need to look at a custom decoder? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# January 10, 2019\n",
    "## Binary data in python\n",
    "Starting to understand how python handles binary data. It's mostly strings. I can use the struct method to unpack the data into the expected 32bit words. I created a test file that only contains a single 4 word header. The expected output is <540717, 123456789, 26001, 2345>. We'll use the [struct](https://docs.python.org/3/library/struct.html#examples) class to help us unpack these into integers that we can deal with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540717\n",
      "123456789\n",
      "26001\n",
      "2345\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "with open(\"../kafka/testfile.bin\", \"rb\") as f:\n",
    "    while True:\n",
    "        chunk = f.read(4)\n",
    "        if chunk:\n",
    "            print(struct.unpack('i', chunk)[0])\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with in memory binary data uses the `io.bytesIO` class. Let's try applying that to the data from kafka. Didn't get us anywhere meaningful. I'm going to try just sending the 4word unittest header through Kafka and see what comes out the other side. That should be much simpler to deal with. \n",
    "\n",
    "Looks like we've managed to get the data out properly? I'm not sure how this corresponds to the words, but things seem to be the same coming through Kafka.\n",
    "\n",
    "| C++        | Python     |\n",
    "|------------|------------|\n",
    "| 1400106115 | 1400106115 |\n",
    "| 3          | 3          |\n",
    "| 8126945    | 8126945    |\n",
    "| 30540247   | 30540247   |\n",
    "| 30736852   | 30736852   |\n",
    "| 30147030   | 30147030   |\n",
    "| 30409163   | 30409163   |\n",
    "| 30867925   | 30867925   |\n",
    "| 30540241   | 30540241   |\n",
    "| 31523292   | 31523292   |\n",
    "| 30671325   | 30671325   |\n",
    "| 30736852   | 30736852   |\n",
    "| 30474713   | 30474713   |\n",
    "| 29426117   | 29426117   |\n",
    "| 29950400   | 29950400   |\n",
    "| 30933459   | 30933459   |\n",
    "| 30933464   | 30933464   |\n",
    "| 30474713   | 30474713   |\n",
    "| 29950416   | 29950416   |\n",
    "| 30605774   | 30605774   |\n",
    "| 30409168   | 30409168   |\n",
    "| 31588831   | 31588831   |\n",
    "| 31195614   | 31195614   |\n",
    "| 29557204   | 29557204   |\n",
    "| 30147015   | 30147015   |\n",
    "| 30605779   | 30605779   |\n",
    "| 30999000   | 30999000   |\n",
    "\n",
    "I'm outputting the buffer before converting to a string for sending w/ Kafka: \n",
    "```c++\n",
    "for (unsigned int i = 0; i < bufLen; i++)\n",
    "            std::cout << buf[i] << std::endl;\n",
    "```\n",
    "This is being decoded in the consumer with \n",
    "```python\n",
    "b = io.BytesIO(msg.value())\n",
    "with b as buffer:\n",
    "    while True:\n",
    "        chunk = buffer.read(4)\n",
    "        if chunk:\n",
    "            print(struct.unpack('i', chunk)[0])\n",
    "        else:\n",
    "            break\n",
    "```\n",
    "\n",
    "Interestingly, the buffer doesn't contain an integer number of words. I'm not entirely sure how this happens with this generated file. Might be indicative of problems unpacking data. \n",
    "\n",
    "Success! We sent over the encoded unittest vector containing just the 4 word header and got back the appropriate response: \n",
    "```\n",
    "got message and sending to output file now\n",
    "540717\n",
    "123456789\n",
    "26001\n",
    "2345\n",
    "```\n",
    "I suspect an issue with the data file. \n",
    "## Comparing data formats\n",
    "The binary data is more efficient than the JSON structure. This surprised nobody ever. The binary data weighs in at 16 bytes. JSON at 426 bytes. That's an increase of about 27x. This is certianly a more efficient packing structure than for storage, but terrible for quick access (e.g. analysis). JSON compress to 250 bytes.  Compression factor of 1.7. This gives us only an increase of 16x from the binary. We'll have to consider that the data could be replicated as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# January 11, 2019\n",
    "## Reviewing data file. \n",
    "I've created the following data file \n",
    "```\n",
    "48 45 41 44 01 00 00 00 04 00 00 00 00 00 B8 41 50 49 58 49 45 20 4C 49 53 54 20 44 41 54 41 20 55 20 4F 46 20 54 45 4E 4E 45 53 53 45 45 20 20 54 68 75 20 4A 61 6E 20 31 32 20 31 31 3A 33 39 3A 33 32 20 32 30 31 37 54 68 75 20 4A 61 6E 20 31 32 20 31 31 3A 33 39 3A 35 35 20 32 30 31 37 10 00 00 00 50 49 58 49 45 20 64 61 74 61 20 66 69 6C 65 20 FF FF FF FF 44 41 54 41 04 00 00 00 00 00 00 00 2D 40 08 00 15 CD 5B 07 91 65 00 00 29 09 00 00 45 4F 46 20 FF FF FF FF\n",
    "```\n",
    "\n",
    "It's about as simple as I can get. A HEAD buffer, with a single DATA buffer, then the end of the file. Sadly, `utkscan` cannot read the file. It's decoding the buffer, and then completely missing the DATA buffer. How the code actually unpacks anything properly is a complete mystery to me. \n",
    "\n",
    "This is getting wild. Looks like it's iterating past the first word of the DATA buffer (the length), and really only passing along the VSN. I'm debating on how far down this rabbit hole I should go. In principle, I should be able to get all of this information directly from the event. We just iterate through the file, find \"DATA\" find \"0xFFFFFFFF\" and everything between that goes into a message buffer. That sounds like the most straight forward way to go about getting these files converted. If we have corrupted data we'll have to sort out how to handle that. \n",
    "\n",
    "## Python data reader\n",
    "Now that we know we can handle the data in python, we'll just move on to creating a file reader for python. We'll read everything between `DATA` and `0xFFFFFFFF` into a Kafka message then get on with our lives. This method should work for LDFs and PLDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# January 12, 2019\n",
    "## Data reader continued\n",
    "We're able to read data files pretty reliably now. \n",
    "\n",
    "---\n",
    "### D:/data/svp/kafka-tests/kafka-data-test-0.pld\n",
    "* Number of Dirs         :  0 \n",
    "* Number of Head         :  1 \n",
    "* Number of Data Blocks  :  1 \n",
    "* Number of Buffer Pads  :  3 \n",
    "* Number of End of Files :  1 \n",
    "* Total Words in File    :  41 \n",
    "* Time To Read (s)       :  0.0\n",
    "\n",
    "### D:/data/svp/kafka-tests/bagel-single-spill.pld\n",
    "* Number of Dirs         :  0 \n",
    "* Number of Head         :  1 \n",
    "* Number of Data Blocks  :  1 \n",
    "* Number of Buffer Pads  :  3 \n",
    "* Number of End of Files :  1 \n",
    "* Total Words in File    :  223456 \n",
    "* Time To Read (s)       :  0.24099326133728027\n",
    "\n",
    "### D:/data/utk/pixieworkshop/pulser_003.ldf\n",
    "* Number of Dirs         :  1 \n",
    "* Number of Head         :  1 \n",
    "* Number of Data Blocks  :  193 \n",
    "* Number of Buffer Pads  :  31215 \n",
    "* Number of End of Files :  2 \n",
    "* Total Words in File    :  1614219 \n",
    "* Time To Read (s)       :  1.7529993057250977\n",
    "\n",
    "### D:/data/ithemba/bagel/runs/runBaGeL_337.pld\n",
    "* Number of Dirs         :  0 \n",
    "* Number of Head         :  1 \n",
    "* Number of Data Blocks  :  1204 \n",
    "* Number of Buffer Pads  :  1206 \n",
    "* Number of End of Files :  1 \n",
    "* Total Words in File    :  282727801 \n",
    "* Time To Read (s)       :  303.4150004386902\n",
    "\n",
    "### D:/data/anl/vandle2015/a135feb_12.ldf\n",
    "* Number of Dirs         :  1 \n",
    "* Number of Head         :  1 \n",
    "* Number of Data Blocks  :  65498 \n",
    "* Number of Buffer Pads  :  22960597 \n",
    "* Number of End of Files :  16386 \n",
    "* Total Words in File    :  536731583 \n",
    "* Time To Read (s)       :  571.7240064144135\n",
    "\n",
    "---\n",
    "For some of the longer files it takes about 5-7 minutes to scan through them. Keep in mind that we're just scanning word by word (4 bits per read). We can make the LDFs more efficient by increasing the chunk size to 8192, which is the fixed size of an LDF buffer. From what I can tell the PLD files have three words before the actual data starts. I'm trying to determine what these words are. \n",
    "\n",
    "## Moving data \n",
    "I'm moving the data from WSL to native windows install of postgres. I'm getting tired of popping open the WSL just to start postgres. It now starts automatically. \n",
    "\n",
    "## Import sql file into postgres\n",
    "```sql\n",
    "postgres=# \\i c:/data/data01.sql\n",
    "```\n",
    "\n",
    "## Running Join Query\n",
    "I'm trying out this join to see if we can get some results out of it. Here's the query: \n",
    "```sql\n",
    "SELECT \n",
    "  id00.time, \n",
    "  id00.energy, \n",
    "  id10.time, \n",
    "  id10.energy\n",
    "FROM \n",
    "  id10 \n",
    "JOIN id00 ON id10.time BETWEEN id00.time+63 and id00.time+126\n",
    "ORDER BY \n",
    "  id00.time;\n",
    "```\n",
    "It's been running for 28 minutes with no results yet. Will let it continue to roll and see what we get. It never finished..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# January 13, 2019\n",
    "## Looking at PLD files \n",
    "From what I can tell the first three words are supposed to be the number of words in the buffer and then the data. I'm tracking down where \"data\" gets set.  Really, I guess we don't have to worry about this. It seems that all the PLDs are consistent in their formatting. 3 junk words then real data starts up. Here are the words from a few files. It does look like the newer versions of the writer are writing something different than the old ones. There's a reason that this is listed as an \"experimental\" format.  \n",
    "\n",
    "* timing_003.pld\n",
    "  * DATA\n",
    "    * 65610\n",
    "    * 2\n",
    "    * 0\n",
    "* runPR270_482.pld\n",
    "  * DATA\n",
    "    * 65088\n",
    "    * 65088\n",
    "    * 0\n",
    "* lowgain_001.pld\n",
    "  * DATA\n",
    "    * 65542\n",
    "    * 65540\n",
    "    * 0\n",
    "* lowgain_003.pld\n",
    "  * DATA\n",
    "    * 65542\n",
    "    * 65540\n",
    "    * 0\n",
    "* runBaGeL_337.pld\n",
    "  * DATA\n",
    "    * 223420\n",
    "    * 65542\n",
    "    * 0\n",
    "  * DATA\n",
    "    * 226332\n",
    "    * 65542\n",
    "    * 0\n",
    "  * DATA\n",
    "    * 230776\n",
    "    * 58286\n",
    "    * 0\n",
    "    \n",
    "**Conclusion: We can use the idea of skipping three words before getting to the data for all PLD files.** \n",
    "\n",
    "What about LDF files?\n",
    "* pulser_003.ldf\n",
    "  * DATA\n",
    "    * 8192\n",
    "    * 32764\n",
    "    * 10\n",
    "    * 1\n",
    "* sipmSkutek_001.ldf\n",
    "  * DATA\n",
    "    * 8192\n",
    "    * 32748\n",
    "    * 10\n",
    "    * 0\n",
    "  * DATA\n",
    "    * 8192\n",
    "    * 32748\n",
    "    * 10\n",
    "    * 1\n",
    "\n",
    "Looks like LDFs skip 4 words. The fourth word is pretty useful since it tells us which chunk number we're working with. This is useful since LDFs can potentially split spills between buffers. Though, we never seem to actually do that. There's always some padding between the data and the end of the buffer (e.g. more than one `0xFFFFFFFF`). \n",
    "\n",
    "## Starting to build out Pixie Data Masks\n",
    "I'm creating a class that will provide us decoding ability on the Pixie16 List Mode Data. The mask class is up and running properly, still need to write the unittests for it. \n",
    "\n",
    "## Bagel data\n",
    "I'm looking at the first DATA block in the bagel_337 file and see some irregularities in the data. We're seeing a jump from everything looking good to everything being foobar. \n",
    "```json\n",
    "{\n",
    "  \"crate\": 0,\n",
    "  \"slot\": 2,\n",
    "  \"channel\": 14,\n",
    "  \"header_length\": 4,\n",
    "  \"event_length\": 4,\n",
    "  \"finish_code\": 0,\n",
    "  \"event_time_low\": 857012604,\n",
    "  \"event_time_high\": 4565,\n",
    "  \"cfd_fractional_time\": 7992,\n",
    "  \"cfd_trigger_source_bit\": 1,\n",
    "  \"cfd_forced_trigger_bit\": 0,\n",
    "  \"energy\": 74,\n",
    "  \"trace_length\": 0,\n",
    "  \"trace_out_of_range\": 0\n",
    "},\n",
    "{\n",
    "  \"crate\": 15,\n",
    "  \"slot\": 3,\n",
    "  \"channel\": 6,\n",
    "  \"header_length\": 12,\n",
    "  \"event_length\": 0,\n",
    "  \"finish_code\": 0,\n",
    "  \"event_time_low\": 1,\n",
    "  \"event_time_high\": 16436,\n",
    "  \"cfd_fractional_time\": 8,\n",
    "  \"cfd_trigger_source_bit\": 0,\n",
    "  \"cfd_forced_trigger_bit\": 0,\n",
    "  \"energy\": 65,\n",
    "  \"trace_length\": 6659,\n",
    "  \"trace_out_of_range\": 0\n",
    "},\n",
    "```\n",
    "As you can see we jump from crate 0 to crate 15. This makes no sense at all. I'm surprised that PAASS doesn't complain about this. Here's the encoded data:\n",
    "> 2C 40 08 00 | B7 B4 13 33 | D5 11 B7 39 | 9E 00 00 00 \n",
    "\n",
    "> 2A 40 08 00 | 6C 04 14 33 | D5 11 29 0E | ED 00 00 00 \n",
    "\n",
    "> 2C 40 08 00 | D6 AC 14 33 | D5 11 EB 03 | 5A 00 00 00\n",
    "\n",
    "> 2E 40 08 00 | 7C F9 14 33 | D5 11 38 5F | 4A 00 00 00\n",
    "\n",
    "> 36 CF 00 00 | 01 00 00 00 | 34 40 08 00 | 41 00 03 1A\n",
    "\n",
    "> D5 11 E5 64 | 00 02 00 00 | 33 40 08 00 | 98 96 03 1A\n",
    "\n",
    "> D5 11 B2 71 | 44 00 00 00 | 3B 40 08 00 | 9C 96 03 1A \n",
    "\n",
    "> D5 11 7D 76 | A5 00 00 00 | 37 40 08 00 | 54 C5 03 1A \n",
    "\n",
    "> D5 11 81 5B | B8 02 00 00 | 39 40 08 00 | E3 DD 05 1A\n",
    "\n",
    "> ....\n",
    "\n",
    "> 55 40 08 00 | 99 D3 14 33 | D5 11 24 37 | CD 01 00 00\n",
    "\n",
    "The next data block starts with \n",
    "> 29 40 08 00 | 21 12 15 33 | D5 11 62 57 | 7B 00 00 00\n",
    "\n",
    "Based on where the data block ends up it looks like the data may have been corrupted or otherwise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# January 14, 2019\n",
    "## Digging in a bit more\n",
    "This looks like an event that had an external timestamp on it or something. \n",
    "> 2E 40 08 00 | 7C F9 14 33 | D5 11 38 5F | 4A 00 00 00 | 36 CF 00 00 | 01 00 00 00\n",
    "\n",
    "Bwah! I know what this is. It's the split to the next module!! Instead of breaking the modules into clearly distinct sections, they're packed with the length of the buffer + module number THEN the data. Those extra two words were throwing off my scanning.\n",
    "\n",
    "## Generating postgres table\n",
    "Got everything hooked up properly now. We're ready to start feeding these decoded data into the postgres database. I created a table from a simple header:\n",
    "```sql\n",
    "CREATE TABLE public.run337\n",
    "(\n",
    "  channel smallint,\n",
    "  slot smallint,\n",
    "  crate smallint,\n",
    "  header_length smallint,\n",
    "  event_length smallint,\n",
    "  finish_code boolean,\n",
    "  event_time_low bigint,\n",
    "  event_time_high integer,\n",
    "  cfd_fractional_time integer,\n",
    "  cfd_trigger_source_bit boolean,\n",
    "  cfd_forced_trigger_bit boolean,\n",
    "  energy integer,\n",
    "  trace_length smallint,\n",
    "  trace_out_of_range boolean\n",
    ")\n",
    "```\n",
    "I tried to match the data types to the minimum that they would fit. Postgres doesn't have an unsigned integer type (LAME), so we'll have to store `event_time_low` as a bigint. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dolosse",
   "language": "python",
   "name": "dolosse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
